from __future__ import print_function
import os
import sys
import torch 
import torch.nn as nn
import numpy as np
import torch.nn.functional as F
import math, copy, time
from torch.autograd import Variable
import matplotlib.pyplot as plt
import argparse
import time
import collections

from models import RNN, GRU 
from models import make_model as TRANSFORMER

# ARG PARSING AND EXPERIMENT SETUP
parser = argparse.ArgumentParser(description='PyTorch Penn Treebank Language Modeling')
# Arguments you may need to set to run different experiments in 4.1 & 4.2.
parser.add_argument('--data', type=str, default='data',
                    help='location of the data corpus')
parser.add_argument('--model', type=str, default='GRU',
                    help='type of recurrent net (RNN, GRU, TRANSFORMER)')
parser.add_argument('--optimizer', type=str, default='SGD_LR_SCHEDULE',
                    help='optimization algo to use; SGD, SGD_LR_SCHEDULE, ADAM')
parser.add_argument('--seq_len', type=int, default=35,
                    help='number of timesteps over which BPTT is performed')
parser.add_argument('--batch_size', type=int, default=20,
                    help='size of one minibatch')
parser.add_argument('--initial_lr', type=float, default=20.0,
                    help='initial learning rate')
parser.add_argument('--hidden_size', type=int, default=200,
                    help='size of hidden layers. IMPORTANT: for the transformer\
                    this must be a multiple of 16.')
parser.add_argument('--save_best', action='store_true',
                    help='save the model for the best validation performance')
parser.add_argument('--num_layers', type=int, default=2,
                    help='number of hidden layers in RNN/GRU, or number of transformer blocks in TRANSFORMER')
# Other hyperparameters you may want to tune in your exploration
parser.add_argument('--emb_size', type=int, default=200,
                    help='size of word embeddings')
parser.add_argument('--num_epochs', type=int, default=40,
                    help='number of epochs to stop after')
parser.add_argument('--dp_keep_prob', type=float, default=0.35,
                    help='dropout *keep* probability. drop_prob = 1-dp_keep_prob \
                    (dp_keep_prob=1 means no dropout)')
# Arguments that you may want to make use of / implement more code for
parser.add_argument('--debug', action='store_true') 
parser.add_argument('--save_dir', type=str, default='',
                    help='path to save the experimental config, logs, model \
                    This is automatically generated based on the command line \
                    arguments you pass and only needs to be set if you want a \
                    custom dir name')
parser.add_argument('--evaluate', action='store_true',
                    help="use this flag to run on the test set. Only do this \
                    ONCE for each model setting, and only after you've \
                    completed ALL hyperparameter tuning on the validation set.\
                    Note we are not requiring you to do this.")
# DO NOT CHANGE THIS (setting the random seed makes experiments deterministic, 
# which helps for reproducibility)
parser.add_argument('--seed', type=int, default=1111,
                    help='random seed')
args = parser.parse_args()
argsdict = args.__dict__
argsdict['code_file'] = sys.argv[0]
torch.manual_seed(args.seed)
device = torch.device("cpu")


#Setting arguments
def setattrs(_self, **kwargs):
    for k,v in kwargs.items():
        setattr(_self, k, v)

#The 3 architectures for 4.1
setattrs(args, model= 'RNN', optimizer='ADAM', initial_lr=0.0001, batch_size=20, seq_len=35, hidden_size=1500, num_layers=2, dp_keep_prob=0.35)
setattrs(args, model= 'GRU', optimizer='SGD_LR_SCHEDULE', initial_lr=10, batch_size=20, seq_len=35, hidden_size=1500, num_layers=2, dp_keep_prob=0.35)
setattrs(args, model='TRANSFORMER', optimizer='SGD_LR_SCHEDULE', initial_lr=20, batch_size=128, seq_len=35, hidden_size=512, num_layers=6, dp_keep_prob=0.9)

#The 6 architectures for 4.2
setattrs(args, model='RNN', optimizer='SGD', initial_lr=0.0001, batch_size=20, seq_len=35, hidden_size=1500, num_layers=2, dp_keep_prob=0.35)
setarttr(args, model='RNN', optimizer='SGD_LR_SCHEDULE', initial_lr=1, batch_size=20, seq_len=35, hidden_size=512, num_layers=2, dp_keep_prob=0.35)

setattrs(args, model='GRU', optimizer='SGD', initial_lr=10, batch_size=20, seq_len=35, hidden_size=1500, num_layers=2, dp_keep_prob=0.35)
setattrs(args, model='GRU', optimizer='ADAM', initial_lr=0.0001, batch_size=20, seq_len=35, hidden_size=1500, num_layers=2, dp_keep_prob=0.35)

setattrs(args, model='TRANSFORMER', optimizer='SGD', initial_lr=20, batch_size=128, seq_len=35, hidden_size=512, num_layers=6, dp_keep_prob=.9)
setattrs(args, model='TRANSFORMER', optimizer='ADAM', initial_lr=0.001, batch_size=128, seq_len=35, hidden_size=512, num_layers=2, dp_keep_prob=.9)

if args.model == 'RNN':
    model = RNN(emb_size=args.emb_size, hidden_size=args.hidden_size, 
                seq_len=args.seq_len, batch_size=args.batch_size,
                vocab_size=vocab_size, num_layers=args.num_layers, 
                dp_keep_prob=args.dp_keep_prob) 
elif args.model == 'GRU':
    model = GRU(emb_size=args.emb_size, hidden_size=args.hidden_size, 
                seq_len=args.seq_len, batch_size=args.batch_size,
                vocab_size=vocab_size, num_layers=args.num_layers, 
                dp_keep_prob=args.dp_keep_prob)
elif args.model == 'TRANSFORMER':
    if args.debug:  # use a very small model
        model = TRANSFORMER(vocab_size=vocab_size, n_units=16, n_blocks=2)
    else:
        # Note that we're using num_layers and hidden_size to mean slightly 
        # different things here than in the RNNs.
        # Also, the Transformer also has other hyperparameters 
        # (such as the number of attention heads) which can change it's behavior.
        model = TRANSFORMER(vocab_size=vocab_size, n_units=args.hidden_size, 
                            n_blocks=args.num_layers, dropout=1.-args.dp_keep_prob) 
    # these 3 attributes don't affect the Transformer's computations; 
    # they are only used in run_epoch
    model.batch_size=args.batch_size
    model.seq_len=args.seq_len
    model.vocab_size=vocab_size
else:
  print("Model type not recognized.")

model = model.to(device)
