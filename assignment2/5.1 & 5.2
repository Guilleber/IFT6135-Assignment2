from __future__ import print_function
import torch 
import torch.nn as nn
import numpy as np
import torch.nn.functional as F
import math, copy, time
from torch.autograd import Variable
import matplotlib.pyplot as plt

from models import RNN, GRU 
from models import make_model as TRANSFORMER


########################### 5.2 ###########################################################

def average_Lt(model, data, is_train=False, lr=1.0):

    "Getting the data into a matrix where each column represents a batch"   
    raw_data = np.array(validation_data, dtype=np.int32)
    data_len = len(raw_data)
    batch_len = data_len // args.batch_size
    data = np.zeros([args.batch_size, batch_len], dtype=np.int32)
    for i in range(args.batch_size):
        data[i] = raw_data[batch_len * i:batch_len * (i + 1)]

    num_steps = args.seq_len
    epoch_size = (batch_len - 1) // num_steps
    "To have the cross entropy loss for each timestep we cant replace nn.crossentropyloss by these two equivalent functions"
    sm = nn.LogSoftmax()
    nll = nn.NLLLoss()

    loss_moy = torch.zeros(num_steps)
    n = 0
    for i in range(epoch_size):
        x = data[:, i*num_steps:(i+1)*num_steps]
        y = data[:, i*num_steps+1:(i+1)*num_steps+1]

        inputs = torch.from_numpy(x.astype(np.int64)).transpose(0, 1).contiguous().to(device)#.cuda() nn.CrossEntropyLoss(outputs, targets)
        hidden = model.init_hidden()
        hidden = hidden.to(device)
        hidden = repackage_hidden(hidden)

        outputs, hidden = model(inputs, hidden)
        targets = torch.from_numpy(y.astype(np.int64)).transpose(0, 1).contiguous().to(device)#.cuda()

        for l in range(num_steps):
            for k in range(args.batch_size):
                o = (outputs[l,k,:]).reshape(1,10000)
                t = (targets[l,k]).reshape(1)
                loss_moy[l] += (nll(sm(o), t)).data.item()
                n+=1
    loss_moy = loss_moy/n

    plt.plot(list(range(1,36)), list(loss_moy), 'go-', label='line 1', linewidth=2)
    plt.title('Average loss at each timestep')
    plt.xlabel('t')
    plt.ylabel('L_t')
    plot = plt
    #plt.savefig('average_time_ste_loss.png')

    return loss_moy, plot, n #the n is to verify if all of the data has been swept through


########################### 5.2 ###########################################################

def grad_mean_norm(model, data, is_train=False, lr=1.0):

    #Load one mini batch
    raw_data = np.array(valid_data, dtype=np.int32)
    mini_batch = np.zeros([args.batch_size, args.seq_len], dtype=np.int32)
    for i in range(args.batch_size):
        mini_batch[i] = raw_data[0:args.seq_len]  
        
    model.eval()

    if args.model != 'TRANSFORMER':
        hidden = model.init_hidden()
        hidden = hidden.to(device)

        #extract weight and hidden values of last layer
        l = args.batch_size*args.seq_len
        grad_moy = torch.zeros(args.hidden_size, vocab_size)
        grad_norm = torch.zeros(args.batch_size)
        w_last = model.linears_out[1].weight
        h_last = hidden[1,:,:] 
    
    for t in range(args.batch_size):
        h_t = h_last[t,:]
        exp_term = torch.exp(torch.mm(w_last,h_t.reshape(args.hidden_size,1)))
        grad_seq =  torch.mm(torch.t(w_last), exp_term) - torch.t(w_last)
        grad_norm[t] = torch.norm(grad_seq, p='fro', dim=None, keepdim=False, out=None).data.item() # L2 norm is default
        grad_moy += grad_seq

    grad_moy = grad_moy / args.batch_size
    
    #plot grad norm
    itr = [i*35 for i in range(1,21)]
    plt.plot(list(itr), list(grad_norm), 'go-', label='line 1', linewidth=2)
    plt.title('l2 norm of loss gradients at timestep t')
    plt.xlabel('t')
    plt.ylabel('gradL_t')
    plot = plt

    return grad_moy, grad_norm, plot 



